"""
å¯¹æ¯”å®éªŒè„šæœ¬ï¼šAutoGen vs åŸºçº¿ç®—æ³•
"""

import json
import time
from datetime import datetime
from baseline_algorithms import run_baseline_algorithm, TaskAllocationProblem
from evaluation_metrics import AllocationEvaluator
import os


class ComparisonExperiments:
    """å¯¹æ¯”å®éªŒç®¡ç†ç±»"""
    
    def __init__(self):
        self.results = {}
        self.output_dir = "comparison_results"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def load_autogen_result(self, filename='output_allocation.json'):
        """åŠ è½½AutoGenç®—æ³•çš„ç»“æœ"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ æœªæ‰¾åˆ°AutoGenç»“æœæ–‡ä»¶: {filename}")
            print("   è¯·å…ˆè¿è¡Œ python autogen_uav_allocation.py ç”Ÿæˆç»“æœ")
            return None
    
    def run_baseline(self, algorithm_name: str) -> dict:
        """è¿è¡ŒåŸºçº¿ç®—æ³•"""
        print(f"\n{'='*70}")
        print(f"è¿è¡Œ {algorithm_name.upper()} ç®—æ³•")
        print('='*70)
        
        start_time = time.time()
        
        problem = TaskAllocationProblem.from_default_scenario()
        result = run_baseline_algorithm(algorithm_name, problem)
        
        runtime = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        output_file = f'{self.output_dir}/allocation_{algorithm_name}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {algorithm_name.upper()} å®Œæˆ")
        print(f"   è¿è¡Œæ—¶é—´: {runtime:.3f} ç§’")
        print(f"   å®Œæˆä»»åŠ¡: {len(result['final_allocation']['assignments'])}/{result['final_allocation']['total_tasks']}")
        print(f"   ä¿å­˜åˆ°: {output_file}")
        
        return {
            'result': result,
            'runtime': runtime,
            'algorithm': algorithm_name
        }
    
    def evaluate_result(self, result: dict, algorithm_name: str) -> dict:
        """è¯„ä¼°å•ä¸ªç®—æ³•çš„ç»“æœ"""
        print(f"\nè¯„ä¼° {algorithm_name.upper()} ç®—æ³•...")
        
        evaluator = AllocationEvaluator(result)
        metrics = evaluator.evaluate_all()
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_file = f'{self.output_dir}/evaluation_{algorithm_name}.json'
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        
        print(f"   æ€»ä½“è¯„åˆ†: {metrics['overall_score']:.2f}/100")
        
        return metrics
    
    def run_all_algorithms(self):
        """è¿è¡Œæ‰€æœ‰ç®—æ³•å¹¶è¯„ä¼°"""
        print("=" * 70)
        print("AutoGen vs åŸºçº¿ç®—æ³•å¯¹æ¯”å®éªŒ")
        print("=" * 70)
        print(f"\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        algorithms = {
            'autogen': None,  # ä»æ–‡ä»¶åŠ è½½
            'greedy': self.run_baseline,
            'random': self.run_baseline,
            'genetic': self.run_baseline,
            'ip': self.run_baseline,
        }
        
        # 1. è¿è¡Œæ‰€æœ‰ç®—æ³•
        for algo_name, run_func in algorithms.items():
            if algo_name == 'autogen':
                print(f"\n{'='*70}")
                print("åŠ è½½ AUTOGEN ç®—æ³•ç»“æœ")
                print('='*70)
                
                result = self.load_autogen_result()
                if result is None:
                    print("âš ï¸ è·³è¿‡AutoGenç®—æ³•ï¼ˆç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰")
                    continue
                
                self.results[algo_name] = {
                    'result': result,
                    'runtime': 0,  # AutoGençš„è¿è¡Œæ—¶é—´éœ€è¦å•ç‹¬è®°å½•
                    'algorithm': algo_name
                }
                print("âœ… AutoGen ç»“æœåŠ è½½æˆåŠŸ")
            else:
                self.results[algo_name] = run_func(algo_name)
        
        # 2. è¯„ä¼°æ‰€æœ‰ç®—æ³•
        print(f"\n{'='*70}")
        print("è¯„ä¼°æ‰€æœ‰ç®—æ³•")
        print('='*70)
        
        for algo_name, data in self.results.items():
            metrics = self.evaluate_result(data['result'], algo_name)
            self.results[algo_name]['metrics'] = metrics
        
        # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()
        
        return self.results
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print("å¯¹æ¯”å®éªŒæŠ¥å‘Š")
        print('='*70)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        print(f"\n{'ç®—æ³•':<15} {'æ€»åˆ†':<10} {'ä»»åŠ¡å®Œæˆç‡':<12} {'æ—¶é—´æ•ˆç‡':<12} {'èµ„æºåˆ©ç”¨':<12} {'çº¦æŸæ»¡è¶³':<12}")
        print('-' * 90)
        
        for algo_name, data in self.results.items():
            metrics = data['metrics']
            print(f"{algo_name.upper():<15} "
                  f"{metrics['overall_score']:<10.2f} "
                  f"{metrics['task_completion']['completion_rate']:<12.1f} "
                  f"{metrics['time_efficiency']['score']*100:<12.1f} "
                  f"{metrics['resource_utilization']['score']*100:<12.1f} "
                  f"{metrics['constraint_satisfaction']['score']*100:<12.1f}")
        
        # ä¿å­˜å®Œæ•´å¯¹æ¯”ç»“æœ
        comparison_data = {
            'experiment_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'algorithms': list(self.results.keys()),
            'results': {}
        }
        
        for algo_name, data in self.results.items():
            comparison_data['results'][algo_name] = {
                'overall_score': data['metrics']['overall_score'],
                'task_completion_rate': data['metrics']['task_completion']['completion_rate'],
                'time_efficiency': data['metrics']['time_efficiency']['score'],
                'resource_utilization': data['metrics']['resource_utilization']['score'],
                'constraint_satisfaction': data['metrics']['constraint_satisfaction']['score'],
                'runtime': data['runtime'],
                'completed_tasks': data['metrics']['task_completion']['completed_tasks'],
                'total_tasks': data['metrics']['task_completion']['total_tasks'],
            }
        
        # ä¿å­˜JSONæ ¼å¼
        report_file = f'{self.output_dir}/comparison_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_algo = max(self.results.items(), 
                       key=lambda x: x[1]['metrics']['overall_score'])
        
        print(f"\nğŸ† æœ€ä½³ç®—æ³•: {best_algo[0].upper()}")
        print(f"   æ€»ä½“è¯„åˆ†: {best_algo[1]['metrics']['overall_score']:.2f}/100")


def run_autogen_vs_greedy():
    """è¿è¡ŒAutoGen vs è´ªå¿ƒç®—æ³•çš„å¯¹æ¯”å®éªŒ"""
    print("=" * 70)
    print("AutoGen vs è´ªå¿ƒç®—æ³•å¯¹æ¯”å®éªŒ")
    print("=" * 70)
    
    exp = ComparisonExperiments()
    
    # 1. åŠ è½½AutoGenç»“æœ
    print("\n1ï¸âƒ£ åŠ è½½AutoGenç®—æ³•ç»“æœ...")
    autogen_result = exp.load_autogen_result()
    
    if autogen_result is None:
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°AutoGenç»“æœ")
        print("   è¯·å…ˆè¿è¡Œ: python autogen_uav_allocation.py")
        return
    
    # 2. è¿è¡Œè´ªå¿ƒç®—æ³•
    print("\n2ï¸âƒ£ è¿è¡Œè´ªå¿ƒç®—æ³•...")
    greedy_data = exp.run_baseline('greedy')
    
    # 3. è¯„ä¼°ä¸¤ç§ç®—æ³•
    print(f"\n{'='*70}")
    print("3ï¸âƒ£ è¯„ä¼°ç®—æ³•æ€§èƒ½")
    print('='*70)
    
    autogen_metrics = exp.evaluate_result(autogen_result, 'autogen')
    greedy_metrics = exp.evaluate_result(greedy_data['result'], 'greedy')
    
    # 4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*70}")
    print("4ï¸âƒ£ å¯¹æ¯”ç»“æœ")
    print('='*70)
    
    print(f"\n{'æŒ‡æ ‡':<25} {'AutoGen':<15} {'è´ªå¿ƒç®—æ³•':<15} {'å·®å¼‚':<15}")
    print('-' * 70)
    
    # æ€»ä½“è¯„åˆ†
    diff = autogen_metrics['overall_score'] - greedy_metrics['overall_score']
    print(f"{'æ€»ä½“è¯„åˆ†':<25} "
          f"{autogen_metrics['overall_score']:<15.2f} "
          f"{greedy_metrics['overall_score']:<15.2f} "
          f"{diff:+.2f}")
    
    # ä»»åŠ¡å®Œæˆç‡
    ac_rate = autogen_metrics['task_completion']['completion_rate']
    gc_rate = greedy_metrics['task_completion']['completion_rate']
    diff = ac_rate - gc_rate
    print(f"{'ä»»åŠ¡å®Œæˆç‡ (%)':<25} "
          f"{ac_rate:<15.1f} "
          f"{gc_rate:<15.1f} "
          f"{diff:+.1f}")
    
    # æ—¶é—´æ•ˆç‡
    at_eff = autogen_metrics['time_efficiency']['score'] * 100
    gt_eff = greedy_metrics['time_efficiency']['score'] * 100
    diff = at_eff - gt_eff
    print(f"{'æ—¶é—´æ•ˆç‡':<25} "
          f"{at_eff:<15.1f} "
          f"{gt_eff:<15.1f} "
          f"{diff:+.1f}")
    
    # èµ„æºåˆ©ç”¨
    ar_util = autogen_metrics['resource_utilization']['score'] * 100
    gr_util = greedy_metrics['resource_utilization']['score'] * 100
    diff = ar_util - gr_util
    print(f"{'èµ„æºåˆ©ç”¨':<25} "
          f"{ar_util:<15.1f} "
          f"{gr_util:<15.1f} "
          f"{diff:+.1f}")
    
    # çº¦æŸæ»¡è¶³
    ac_sat = autogen_metrics['constraint_satisfaction']['score'] * 100
    gc_sat = greedy_metrics['constraint_satisfaction']['score'] * 100
    diff = ac_sat - gc_sat
    print(f"{'çº¦æŸæ»¡è¶³':<25} "
          f"{ac_sat:<15.1f} "
          f"{gc_sat:<15.1f} "
          f"{diff:+.1f}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_result = {
        'experiment_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'algorithms': ['autogen', 'greedy'],
        'autogen': {
            'overall_score': autogen_metrics['overall_score'],
            'task_completion_rate': ac_rate,
            'time_efficiency': at_eff,
            'resource_utilization': ar_util,
            'constraint_satisfaction': ac_sat,
            'metrics': autogen_metrics
        },
        'greedy': {
            'overall_score': greedy_metrics['overall_score'],
            'task_completion_rate': gc_rate,
            'time_efficiency': gt_eff,
            'resource_utilization': gr_util,
            'constraint_satisfaction': gc_sat,
            'runtime': greedy_data['runtime'],
            'metrics': greedy_metrics
        },
        'comparison': {
            'score_difference': autogen_metrics['overall_score'] - greedy_metrics['overall_score'],
            'winner': 'autogen' if autogen_metrics['overall_score'] > greedy_metrics['overall_score'] else 'greedy'
        }
    }
    
    # ä¿å­˜ç»“æœ
    output_file = f'{exp.output_dir}/autogen_vs_greedy_comparison.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(comparison_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»“è®º
    print(f"\n{'='*70}")
    print("ğŸ“Š ç»“è®º")
    print('='*70)
    
    winner = comparison_result['comparison']['winner']
    score_diff = abs(comparison_result['comparison']['score_difference'])
    
    if winner == 'autogen':
        print(f"ğŸ† AutoGenç®—æ³•è¡¨ç°æ›´ä¼˜ï¼Œé¢†å…ˆ {score_diff:.2f} åˆ†")
    else:
        print(f"âš ï¸ è´ªå¿ƒç®—æ³•è¡¨ç°æ›´ä¼˜ï¼Œé¢†å…ˆ {score_diff:.2f} åˆ†")
    
    return comparison_result


if __name__ == "__main__":
    # è¿è¡ŒAutoGen vs è´ªå¿ƒç®—æ³•å¯¹æ¯”
    result = run_autogen_vs_greedy()
    
    print("\n" + "="*70)
    print("âœ¨ å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print("="*70)
    print("\næŸ¥çœ‹ç»“æœ:")
    print("  â€¢ comparison_results/autogen_vs_greedy_comparison.json")
    print("  â€¢ comparison_results/allocation_greedy.json")
    print("  â€¢ comparison_results/evaluation_autogen.json")
    print("  â€¢ comparison_results/evaluation_greedy.json")
