"""
æ— äººæœºä»»åŠ¡åˆ†é…è¯„ä¼°æŒ‡æ ‡æ¨¡å—
æä¾›å…¨é¢çš„å®šé‡è¯„ä¼°æŒ‡æ ‡è®¡ç®—
"""

import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
import numpy as np


class AllocationEvaluator:
    """æ— äººæœºä»»åŠ¡åˆ†é…æ–¹æ¡ˆè¯„ä¼°å™¨"""
    
    def __init__(self, allocation: Dict, task_input: Dict = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            allocation: åˆ†é…æ–¹æ¡ˆJSON
            task_input: åŸå§‹ä»»åŠ¡è¾“å…¥ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´è¯¦ç»†çš„è¯„ä¼°ï¼‰
        """
        self.allocation = allocation.get('final_allocation', allocation)
        self.task_input = task_input or self._parse_default_input()
        
    def _parse_default_input(self) -> Dict:
        """è§£æé»˜è®¤ä»»åŠ¡è¾“å…¥ï¼ˆä»å†…ç½®æ•°æ®ï¼‰"""
        return {
            'total_tasks': 5,
            'total_uavs': 4,
            'tasks': [
                {'task_id': 'T1', 'priority': 'é«˜', 'time_window': {'start': '08:00', 'end': '10:00'}, 'estimated_duration': 30},
                {'task_id': 'T2', 'priority': 'é«˜', 'time_window': {'start': '08:30', 'end': '09:30'}, 'estimated_duration': 40},
                {'task_id': 'T3', 'priority': 'ä¸­', 'time_window': {'start': '09:00', 'end': '12:00'}, 'estimated_duration': 60},
                {'task_id': 'T4', 'priority': 'ç´§æ€¥', 'time_window': {'start': '08:00', 'end': '08:30'}, 'estimated_duration': 20},
                {'task_id': 'T5', 'priority': 'ä½', 'time_window': {'start': '10:00', 'end': '12:00'}, 'estimated_duration': 30},
            ],
            'uavs': [
                {'uav_id': 'UAV-001', 'max_speed': 80, 'battery': 100},
                {'uav_id': 'UAV-002', 'max_speed': 60, 'battery': 85},
                {'uav_id': 'UAV-003', 'max_speed': 70, 'battery': 90},
                {'uav_id': 'UAV-004', 'max_speed': 65, 'battery': 95},
            ]
        }
    
    def evaluate_all(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢è¯„ä¼°ï¼Œè¿”å›æ‰€æœ‰æŒ‡æ ‡"""
        metrics = {
            'task_completion': self.evaluate_task_completion(),
            'time_efficiency': self.evaluate_time_efficiency(),
            'resource_utilization': self.evaluate_resource_utilization(),
            'constraint_satisfaction': self.evaluate_constraint_satisfaction(),
            'overall_score': 0.0
        }
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        metrics['overall_score'] = self.calculate_overall_score(metrics)
        
        return metrics
    
    def evaluate_task_completion(self) -> Dict[str, Any]:
        """è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦æŒ‡æ ‡"""
        assignments = self.allocation.get('assignments', [])
        unassigned = self.allocation.get('unassigned_tasks', [])
        
        total_tasks = self.task_input['total_tasks']
        completed_tasks = len(assignments)
        
        # æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡
        priority_stats = {
            'ç´§æ€¥': {'total': 0, 'completed': 0},
            'é«˜': {'total': 0, 'completed': 0},
            'ä¸­': {'total': 0, 'completed': 0},
            'ä½': {'total': 0, 'completed': 0}
        }
        
        for task in self.task_input['tasks']:
            priority = task.get('priority', 'ä¸­')
            if priority in priority_stats:
                priority_stats[priority]['total'] += 1
        
        for assignment in assignments:
            priority = assignment.get('priority', 'ä¸­')
            if priority in priority_stats:
                priority_stats[priority]['completed'] += 1
        
        # è®¡ç®—å®Œæˆç‡
        completion_rate = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        # é«˜ä¼˜å…ˆçº§ä»»åŠ¡å®Œæˆç‡
        high_priority_total = priority_stats['ç´§æ€¥']['total'] + priority_stats['é«˜']['total']
        high_priority_completed = priority_stats['ç´§æ€¥']['completed'] + priority_stats['é«˜']['completed']
        high_priority_rate = (high_priority_completed / high_priority_total * 100) if high_priority_total > 0 else 0
        
        return {
            'total_tasks': total_tasks,
            'completed_tasks': completed_tasks,
            'unassigned_tasks': len(unassigned),
            'completion_rate': round(completion_rate, 2),
            'high_priority_completion_rate': round(high_priority_rate, 2),
            'priority_breakdown': priority_stats,
            'score': completion_rate / 100  # å½’ä¸€åŒ–åˆ°0-1
        }
    
    def evaluate_time_efficiency(self) -> Dict[str, Any]:
        """è¯„ä¼°æ—¶é—´æ•ˆç‡æŒ‡æ ‡"""
        assignments = self.allocation.get('assignments', [])
        
        if not assignments:
            return {'score': 0.0}
        
        # è§£ææ—¶é—´
        def parse_time(time_str: str) -> datetime:
            try:
                return datetime.strptime(time_str, '%H:%M')
            except:
                return datetime.strptime('08:00', '%H:%M')
        
        # è®¡ç®—æ€»å®Œæˆæ—¶é—´
        completion_time_str = self.allocation.get('total_completion_time', '12:00')
        start_time = parse_time('08:00')
        end_time = parse_time(completion_time_str)
        total_duration = (end_time - start_time).total_seconds() / 60  # åˆ†é’Ÿ
        
        # è®¡ç®—ä»»åŠ¡ç­‰å¾…æ—¶é—´
        wait_times = []
        for assignment in assignments:
            task_start = parse_time(assignment.get('start_time', '08:00'))
            # å‡è®¾æ‰€æœ‰ä»»åŠ¡æœ€æ—©å¯ä»¥ä»08:00å¼€å§‹
            earliest_start = parse_time('08:00')
            wait_time = (task_start - earliest_start).total_seconds() / 60
            wait_times.append(max(0, wait_time))
        
        avg_wait_time = np.mean(wait_times) if wait_times else 0
        
        # è®¡ç®—ç´§æ€¥ä»»åŠ¡å“åº”æ—¶é—´
        urgent_response_times = []
        for assignment in assignments:
            if assignment.get('priority') == 'ç´§æ€¥':
                task_start = parse_time(assignment.get('start_time', '08:00'))
                window_start = parse_time('08:00')  # T4çš„æ—¶é—´çª—å£å¼€å§‹
                response_time = (task_start - window_start).total_seconds() / 60
                urgent_response_times.append(response_time)
        
        avg_urgent_response = np.mean(urgent_response_times) if urgent_response_times else 0
        
        # æ•ˆç‡åˆ†æ•° (æ€»æ—¶é—´è¶ŠçŸ­è¶Šå¥½)
        max_duration = 240  # 4å°æ—¶
        time_score = max(0, 1 - (total_duration / max_duration))
        
        return {
            'total_completion_time_min': round(total_duration, 2),
            'average_wait_time_min': round(avg_wait_time, 2),
            'urgent_response_time_min': round(avg_urgent_response, 2),
            'time_window_utilization': 0.85,  # ç®€åŒ–è®¡ç®—
            'score': round(time_score, 3)
        }
    
    def evaluate_resource_utilization(self) -> Dict[str, Any]:
        """è¯„ä¼°èµ„æºåˆ©ç”¨æŒ‡æ ‡"""
        assignments = self.allocation.get('assignments', [])
        total_uavs = self.task_input['total_uavs']
        
        # ç»Ÿè®¡æ¯ä¸ªæ— äººæœºçš„ä»»åŠ¡æ•°
        uav_task_count = {}
        for assignment in assignments:
            uav_id = assignment.get('assigned_uav')
            uav_task_count[uav_id] = uav_task_count.get(uav_id, 0) + 1
        
        # ä½¿ç”¨çš„æ— äººæœºæ•°
        used_uavs = len(uav_task_count)
        utilization_rate = (used_uavs / total_uavs * 100) if total_uavs > 0 else 0
        
        # è´Ÿè½½å‡è¡¡åº¦ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå¥½ï¼‰
        task_counts = list(uav_task_count.values())
        if len(task_counts) > 1:
            load_std = np.std(task_counts)
            load_balance_score = max(0, 1 - (load_std / 2))  # æ ‡å‡†å·®å½’ä¸€åŒ–
        else:
            load_std = 0
            load_balance_score = 1.0
        
        # ä¼°ç®—é£è¡Œè·ç¦»ï¼ˆç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªä»»åŠ¡å¹³å‡é£è¡Œ30kmå¾€è¿”ï¼‰
        estimated_distance = len(assignments) * 30
        
        # èµ„æºåˆ©ç”¨åˆ†æ•°
        util_score = (utilization_rate / 100 * 0.5) + (load_balance_score * 0.5)
        
        return {
            'total_uavs': total_uavs,
            'used_uavs': used_uavs,
            'utilization_rate': round(utilization_rate, 2),
            'uav_task_distribution': uav_task_count,
            'load_balance_std': round(load_std, 2),
            'load_balance_score': round(load_balance_score, 3),
            'estimated_total_distance_km': estimated_distance,
            'score': round(util_score, 3)
        }
    
    def evaluate_constraint_satisfaction(self) -> Dict[str, Any]:
        """è¯„ä¼°çº¦æŸæ»¡è¶³æŒ‡æ ‡"""
        assignments = self.allocation.get('assignments', [])
        risk_assessment = self.allocation.get('risk_assessment', '')
        
        # æ£€æµ‹æ—¶é—´å†²çª
        time_conflicts = self._detect_time_conflicts(assignments)
        
        # æ£€æµ‹çº¦æŸè¿åï¼ˆåŸºäºé£é™©è¯„ä¼°æ–‡æœ¬ï¼‰
        constraint_violations = 0
        if 'å†²çª' in risk_assessment or 'è¿å' in risk_assessment:
            constraint_violations += 1
        
        # å®‰å…¨æ€§è¯„åˆ†ï¼ˆåŸºäºé£é™©è¯„ä¼°ï¼‰
        if 'ä½é£é™©' in risk_assessment:
            safety_score = 1.0
        elif 'ä¸­é£é™©' in risk_assessment:
            safety_score = 0.7
        elif 'é«˜é£é™©' in risk_assessment:
            safety_score = 0.4
        else:
            safety_score = 0.8
        
        # çº¦æŸæ»¡è¶³åˆ†æ•°
        conflict_penalty = len(time_conflicts) * 0.2
        violation_penalty = constraint_violations * 0.3
        constraint_score = max(0, 1 - conflict_penalty - violation_penalty)
        
        return {
            'time_conflicts': time_conflicts,
            'conflict_count': len(time_conflicts),
            'constraint_violations': constraint_violations,
            'safety_score': round(safety_score, 3),
            'risk_level': self._extract_risk_level(risk_assessment),
            'score': round(constraint_score, 3)
        }
    
    def _detect_time_conflicts(self, assignments: List[Dict]) -> List[str]:
        """æ£€æµ‹æ—¶é—´å†²çª"""
        conflicts = []
        uav_schedules = {}
        
        for assignment in assignments:
            uav_id = assignment.get('assigned_uav')
            start_time = assignment.get('start_time', '08:00')
            duration_str = assignment.get('estimated_duration', '30åˆ†é’Ÿ')
            
            # è§£ææŒç»­æ—¶é—´
            try:
                duration = int(''.join(filter(str.isdigit, duration_str)))
            except:
                duration = 30
            
            if uav_id not in uav_schedules:
                uav_schedules[uav_id] = []
            
            uav_schedules[uav_id].append({
                'task': assignment.get('task_id'),
                'start': start_time,
                'duration': duration
            })
        
        # æ£€æŸ¥æ¯ä¸ªæ— äººæœºçš„æ—¶é—´è¡¨æ˜¯å¦æœ‰é‡å 
        for uav_id, schedules in uav_schedules.items():
            if len(schedules) > 1:
                # ç®€åŒ–ï¼šå¦‚æœåŒä¸€æ— äººæœºæœ‰å¤šä¸ªä»»åŠ¡ï¼Œæ£€æŸ¥æ—¶é—´æ˜¯å¦åˆç†å®‰æ’
                for i in range(len(schedules) - 1):
                    conflicts.append(f"{uav_id}å¯èƒ½å­˜åœ¨ä»»åŠ¡é—´éš”ç´§å¼ ")
                    break
        
        return conflicts
    
    def _extract_risk_level(self, risk_assessment: str) -> str:
        """ä»é£é™©è¯„ä¼°æ–‡æœ¬æå–é£é™©ç­‰çº§"""
        if 'ä½é£é™©' in risk_assessment:
            return 'ä½'
        elif 'ä¸­é£é™©' in risk_assessment:
            return 'ä¸­'
        elif 'é«˜é£é™©' in risk_assessment:
            return 'é«˜'
        else:
            return 'æœªçŸ¥'
    
    def calculate_overall_score(self, metrics: Dict) -> float:
        """è®¡ç®—åŠ æƒæ€»åˆ†"""
        weights = {
            'task_completion': 0.4,      # ä»»åŠ¡å®Œæˆåº¦æƒé‡æœ€é«˜
            'time_efficiency': 0.25,     # æ—¶é—´æ•ˆç‡
            'resource_utilization': 0.2, # èµ„æºåˆ©ç”¨
            'constraint_satisfaction': 0.15  # çº¦æŸæ»¡è¶³
        }
        
        total_score = 0
        for key, weight in weights.items():
            if key in metrics:
                total_score += metrics[key].get('score', 0) * weight
        
        return round(total_score * 100, 2)  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
    
    def generate_report(self, metrics: Dict) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("=" * 70)
        report.append("ğŸ“Š æ— äººæœºä»»åŠ¡åˆ†é…æ–¹æ¡ˆè¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 70)
        report.append("")
        
        # æ€»ä½“è¯„åˆ†
        report.append(f"ğŸ¯ æ€»ä½“è¯„åˆ†: {metrics['overall_score']}/100")
        report.append("")
        
        # ä»»åŠ¡å®Œæˆåº¦
        tc = metrics['task_completion']
        report.append("1ï¸âƒ£ ä»»åŠ¡å®Œæˆåº¦")
        report.append(f"   â€¢ ä»»åŠ¡å®Œæˆç‡: {tc['completion_rate']}%")
        report.append(f"   â€¢ å®Œæˆä»»åŠ¡æ•°: {tc['completed_tasks']}/{tc['total_tasks']}")
        report.append(f"   â€¢ é«˜ä¼˜å…ˆçº§å®Œæˆç‡: {tc['high_priority_completion_rate']}%")
        report.append(f"   â€¢ è¯„åˆ†: {tc['score']:.3f}")
        report.append("")
        
        # æ—¶é—´æ•ˆç‡
        te = metrics['time_efficiency']
        report.append("2ï¸âƒ£ æ—¶é—´æ•ˆç‡")
        report.append(f"   â€¢ æ€»å®Œæˆæ—¶é—´: {te['total_completion_time_min']:.1f} åˆ†é’Ÿ")
        report.append(f"   â€¢ å¹³å‡ç­‰å¾…æ—¶é—´: {te['average_wait_time_min']:.1f} åˆ†é’Ÿ")
        report.append(f"   â€¢ ç´§æ€¥ä»»åŠ¡å“åº”: {te['urgent_response_time_min']:.1f} åˆ†é’Ÿ")
        report.append(f"   â€¢ è¯„åˆ†: {te['score']:.3f}")
        report.append("")
        
        # èµ„æºåˆ©ç”¨
        ru = metrics['resource_utilization']
        report.append("3ï¸âƒ£ èµ„æºåˆ©ç”¨")
        report.append(f"   â€¢ æ— äººæœºåˆ©ç”¨ç‡: {ru['utilization_rate']:.1f}%")
        report.append(f"   â€¢ ä½¿ç”¨æ— äººæœº: {ru['used_uavs']}/{ru['total_uavs']}")
        report.append(f"   â€¢ è´Ÿè½½å‡è¡¡åˆ†æ•°: {ru['load_balance_score']:.3f}")
        report.append(f"   â€¢ ä»»åŠ¡åˆ†å¸ƒ: {ru['uav_task_distribution']}")
        report.append(f"   â€¢ è¯„åˆ†: {ru['score']:.3f}")
        report.append("")
        
        # çº¦æŸæ»¡è¶³
        cs = metrics['constraint_satisfaction']
        report.append("4ï¸âƒ£ çº¦æŸæ»¡è¶³")
        report.append(f"   â€¢ æ—¶é—´å†²çªæ•°: {cs['conflict_count']}")
        report.append(f"   â€¢ çº¦æŸè¿åæ•°: {cs['constraint_violations']}")
        report.append(f"   â€¢ é£é™©ç­‰çº§: {cs['risk_level']}")
        report.append(f"   â€¢ å®‰å…¨åˆ†æ•°: {cs['safety_score']:.3f}")
        report.append(f"   â€¢ è¯„åˆ†: {cs['score']:.3f}")
        report.append("")
        
        report.append("=" * 70)
        
        return "\n".join(report)


def evaluate_allocation_from_file(json_file: str) -> Tuple[Dict, str]:
    """
    ä»JSONæ–‡ä»¶è¯»å–åˆ†é…æ–¹æ¡ˆå¹¶è¯„ä¼°
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        (metrics, report): è¯„ä¼°æŒ‡æ ‡å­—å…¸å’ŒæŠ¥å‘Šæ–‡æœ¬
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        allocation = json.load(f)
    
    evaluator = AllocationEvaluator(allocation)
    metrics = evaluator.evaluate_all()
    report = evaluator.generate_report(metrics)
    
    return metrics, report


if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°æ¨¡å—
    import sys
    
    if len(sys.argv) > 1:
        json_file = sys.argv[1]
    else:
        json_file = "output_allocation.json"
    
    print(f"æ­£åœ¨è¯„ä¼°åˆ†é…æ–¹æ¡ˆ: {json_file}")
    print()
    
    try:
        metrics, report = evaluate_allocation_from_file(json_file)
        print(report)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        output_file = json_file.replace('.json', '_evaluation.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
