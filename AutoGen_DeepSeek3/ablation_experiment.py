"""
æ™ºèƒ½ä½“æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
éªŒè¯æ¯ä¸ªæ™ºèƒ½ä½“çš„å¿…è¦æ€§å’Œè´¡çŒ®
"""

import asyncio
import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_ext.models.openai import OpenAIChatCompletionClient
from evaluation_metrics import AllocationEvaluator


class AblationExperiment:
    """æ™ºèƒ½ä½“æ¶ˆèå®éªŒç®¡ç†ç±»"""
    
    def __init__(self):
        self.results = {}
        self.output_dir = "ablation_results"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        
        # å®éªŒé…ç½®
        self.configurations = {
            '3-agent': {
                'name': '3æ™ºèƒ½ä½“ï¼ˆæœ€å°é…ç½®ï¼‰',
                'agents': ['TaskAnalyzer', 'SolutionGenerator', 'Arbitrator'],
                'description': 'ç¼ºå°‘èµ„æºè¯„ä¼°å’Œå†²çªæ£€æµ‹',
                'expected': 'åŸºç¡€åˆ†é…èƒ½åŠ›ï¼Œå¯èƒ½èµ„æºåŒ¹é…ä¸åˆç†ä¸”æœ‰å†²çª'
            },
            '4-agent-v1': {
                'name': '4æ™ºèƒ½ä½“-V1ï¼ˆæ— èµ„æºè¯„ä¼°ï¼‰',
                'agents': ['TaskAnalyzer', 'SolutionGenerator', 'ConflictDetector', 'Arbitrator'],
                'description': 'å»æ‰ResourceEvaluator',
                'expected': 'æœ‰å†²çªæ£€æµ‹ä½†èµ„æºåŒ¹é…å¯èƒ½ä¸åˆç†'
            },
            '4-agent-v2': {
                'name': '4æ™ºèƒ½ä½“-V2ï¼ˆæ— å†²çªæ£€æµ‹ï¼‰',
                'agents': ['TaskAnalyzer', 'ResourceEvaluator', 'SolutionGenerator', 'Arbitrator'],
                'description': 'å»æ‰ConflictDetector',
                'expected': 'èµ„æºè¯„ä¼°è‰¯å¥½ä½†å¯èƒ½äº§ç”Ÿå†²çª'
            },
            '5-agent': {
                'name': '5æ™ºèƒ½ä½“ï¼ˆå®Œæ•´é…ç½®-åŸºå‡†ï¼‰',
                'agents': ['TaskAnalyzer', 'ResourceEvaluator', 'SolutionGenerator', 
                          'ConflictDetector', 'Arbitrator'],
                'description': 'å®Œæ•´é…ç½®',
                'expected': 'æœ€ä¼˜è¡¨ç°'
            },
            '6-agent': {
                'name': '6æ™ºèƒ½ä½“ï¼ˆå¢å¼ºé…ç½®ï¼‰',
                'agents': ['TaskAnalyzer', 'ResourceEvaluator', 'SolutionGenerator', 
                          'ConflictDetector', 'PathPlanner', 'Arbitrator'],
                'description': 'å¢åŠ PathPlannerè·¯å¾„è§„åˆ’',
                'expected': 'è·¯å¾„ä¼˜åŒ–èƒ½åŠ›æå‡'
            }
        }
        
        # åŠ è½½é»˜è®¤ä»»åŠ¡æè¿°
        self.task_description = self._load_default_task()
    
    def _load_default_task(self) -> str:
        """åŠ è½½é»˜è®¤ä»»åŠ¡æè¿°"""
        return """
å¤šæ— äººæœºä»»åŠ¡åˆ†é…åœºæ™¯

ã€å¯ç”¨æ— äººæœºã€‘
1. UAV-001ï¼ˆä¾¦å¯Ÿå‹ï¼‰
   - æœ€å¤§é£è¡Œæ—¶é—´ï¼š120åˆ†é’Ÿ
   - æœ€å¤§é€Ÿåº¦ï¼š80 km/h
   - è½½é‡èƒ½åŠ›ï¼š0 kg
   - å½“å‰çŠ¶æ€ï¼šAåŸºåœ°ï¼Œç”µé‡100%

2. UAV-002ï¼ˆè¿è¾“å‹ï¼‰
   - æœ€å¤§é£è¡Œæ—¶é—´ï¼š90åˆ†é’Ÿ
   - æœ€å¤§é€Ÿåº¦ï¼š60 km/h
   - è½½é‡èƒ½åŠ›ï¼š5 kg
   - å½“å‰çŠ¶æ€ï¼šAåŸºåœ°ï¼Œç”µé‡85%

3. UAV-003ï¼ˆä¾¦å¯Ÿå‹ï¼‰
   - æœ€å¤§é£è¡Œæ—¶é—´ï¼š100åˆ†é’Ÿ
   - æœ€å¤§é€Ÿåº¦ï¼š70 km/h
   - è½½é‡èƒ½åŠ›ï¼š0 kg
   - å½“å‰çŠ¶æ€ï¼šBåŸºåœ°ï¼Œç”µé‡90%

4. UAV-004ï¼ˆå¤šç”¨é€”ï¼‰
   - æœ€å¤§é£è¡Œæ—¶é—´ï¼š80åˆ†é’Ÿ
   - æœ€å¤§é€Ÿåº¦ï¼š65 km/h
   - è½½é‡èƒ½åŠ›ï¼š2 kg
   - å½“å‰çŠ¶æ€ï¼šAåŸºåœ°ï¼Œç”µé‡95%

ã€å¾…åˆ†é…ä»»åŠ¡ã€‘
1. T1 - åŒºåŸŸä¾¦å¯Ÿï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
   - æ—¶é—´çª—å£ï¼š08:00-10:00
   - é¢„è®¡ç”¨æ—¶ï¼š30åˆ†é’Ÿ
   - ä½ç½®ï¼šCåŒºåŸŸ

2. T2 - ç‰©èµ„è¿è¾“ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
   - æ—¶é—´çª—å£ï¼š08:30-09:30
   - é¢„è®¡ç”¨æ—¶ï¼š40åˆ†é’Ÿ
   - è½½é‡éœ€æ±‚ï¼š3kg

3. T3 - ç›®æ ‡ç›‘æ§ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰
   - æ—¶é—´çª—å£ï¼š09:00-12:00
   - é¢„è®¡ç”¨æ—¶ï¼š60åˆ†é’Ÿ

4. T4 - ç´§æ€¥ä¾¦å¯Ÿï¼ˆç´§æ€¥ï¼‰
   - æ—¶é—´çª—å£ï¼š08:00-08:30
   - é¢„è®¡ç”¨æ—¶ï¼š20åˆ†é’Ÿ

5. T5 - è®¾å¤‡æŠ•é€ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
   - æ—¶é—´çª—å£ï¼š10:00-12:00
   - é¢„è®¡ç”¨æ—¶ï¼š30åˆ†é’Ÿ
   - è½½é‡éœ€æ±‚ï¼š1kg

ã€çº¦æŸæ¡ä»¶ã€‘
1. ç¦é£åŒºï¼šDåŒºåŸŸåœ¨09:00-09:30ä¸ºä¸´æ—¶ç¦é£åŒº
2. å¹¶å‘é™åˆ¶ï¼šæ¯æ¶æ— äººæœºåŒæ—¶åªèƒ½æ‰§è¡Œä¸€ä¸ªä»»åŠ¡
3. è¿”èˆªè¦æ±‚ï¼šä»»åŠ¡å®Œæˆåéœ€è¿”å›åŸºåœ°

è¯·ä¸ºä»¥ä¸Šä»»åŠ¡ç”Ÿæˆæœ€ä¼˜åˆ†é…æ–¹æ¡ˆã€‚
"""
    
    def _get_llm_client(self):
        """è·å–LLMå®¢æˆ·ç«¯"""
        import os
        from dotenv import load_dotenv
        
        load_dotenv()
        
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError(
                "âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°APIå¯†é’¥ï¼\n"
                "è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­è®¾ç½®äº† DEEPSEEK_API_KEY\n"
                "ä¾‹å¦‚ï¼šDEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxx"
            )
        
        return OpenAIChatCompletionClient(
            model="deepseek-chat",
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
    
    def create_task_analyzer(self) -> AssistantAgent:
        """åˆ›å»ºä»»åŠ¡åˆ†ææ™ºèƒ½ä½“"""
        system_message = """ä½ æ˜¯ä»»åŠ¡åˆ†æä¸“å®¶ã€‚
èŒè´£ï¼šè§£æå’Œåˆ†æä»»åŠ¡éœ€æ±‚ã€‚
è¾“å‡ºï¼š
1. ä»»åŠ¡ä¼˜å…ˆçº§æ’åº
2. å…³é”®çº¦æŸè¯†åˆ«
3. èµ„æºéœ€æ±‚åˆ†æ
ä¿æŒç®€æ´ï¼Œä¸è¦è¾“å‡ºJSONã€‚"""
        
        return AssistantAgent(
            "TaskAnalyzer",
            model_client=self._get_llm_client(),
            system_message=system_message
        )
    
    def create_resource_evaluator(self) -> AssistantAgent:
        """åˆ›å»ºèµ„æºè¯„ä¼°æ™ºèƒ½ä½“"""
        system_message = """ä½ æ˜¯èµ„æºè¯„ä¼°ä¸“å®¶ã€‚
èŒè´£ï¼šè¯„ä¼°æ— äººæœºèƒ½åŠ›å’Œå¯ç”¨æ€§ã€‚
è¾“å‡ºï¼š
1. å„æ— äººæœºçš„èƒ½åŠ›è¯„ä¼°
2. ä»»åŠ¡-æ— äººæœºåŒ¹é…åº¦åˆ†æ
3. èµ„æºç“¶é¢ˆè¯†åˆ«
ä¿æŒç®€æ´ï¼Œä¸è¦è¾“å‡ºJSONã€‚"""
        
        return AssistantAgent(
            "ResourceEvaluator",
            model_client=self._get_llm_client(),
            system_message=system_message
        )
    
    def create_solution_generator(self) -> AssistantAgent:
        """åˆ›å»ºæ–¹æ¡ˆç”Ÿæˆæ™ºèƒ½ä½“"""
        system_message = """ä½ æ˜¯æ–¹æ¡ˆç”Ÿæˆä¸“å®¶ã€‚
èŒè´£ï¼šç”Ÿæˆä»»åŠ¡åˆ†é…æ–¹æ¡ˆã€‚
è¦æ±‚ï¼šå¿…é¡»è¾“å‡ºJSONæ ¼å¼çš„åˆ†é…æ–¹æ¡ˆã€‚
è¾“å‡ºæ ¼å¼ï¼š
{
  "final_allocation": {
    "assignments": [
      {
        "task_id": "T1",
        "task_name": "ä»»åŠ¡å",
        "assigned_uav": "UAV-001",
        "start_time": "08:00",
        "estimated_duration": "30åˆ†é’Ÿ",
        "priority": "é«˜",
        "rationale": "åˆ†é…ç†ç”±"
      }
    ],
    "unassigned_tasks": [],
    "total_completion_time": "10:00"
  }
}
é‡è¦ï¼šæœ€åå¿…é¡»è¾“å‡ºå®Œæ•´JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        return AssistantAgent(
            "SolutionGenerator",
            model_client=self._get_llm_client(),
            system_message=system_message
        )
    
    def create_conflict_detector(self) -> AssistantAgent:
        """åˆ›å»ºå†²çªæ£€æµ‹æ™ºèƒ½ä½“"""
        system_message = """ä½ æ˜¯å†²çªæ£€æµ‹ä¸“å®¶ã€‚
èŒè´£ï¼šæ£€æŸ¥æ–¹æ¡ˆä¸­çš„å†²çªå’Œé—®é¢˜ã€‚
è¾“å‡ºï¼š
1. æ—¶é—´å†²çªæ£€æµ‹
2. èµ„æºå†²çªæ£€æµ‹
3. çº¦æŸè¿åæ£€æµ‹
4. æ”¹è¿›å»ºè®®
ä¿æŒç®€æ´ï¼Œä¸è¦è¾“å‡ºJSONã€‚"""
        
        return AssistantAgent(
            "ConflictDetector",
            model_client=self._get_llm_client(),
            system_message=system_message
        )
    
    def create_path_planner(self) -> AssistantAgent:
        """åˆ›å»ºè·¯å¾„è§„åˆ’æ™ºèƒ½ä½“"""
        system_message = """ä½ æ˜¯è·¯å¾„è§„åˆ’ä¸“å®¶ã€‚
èŒè´£ï¼šä¼˜åŒ–æ— äººæœºé£è¡Œè·¯å¾„ã€‚
è¾“å‡ºï¼š
1. é£è¡Œè·¯å¾„ä¼˜åŒ–å»ºè®®
2. æ—¶é—´ä¼°ç®—ä¼˜åŒ–
3. èƒ½è€—ä¼˜åŒ–å»ºè®®
ä¿æŒç®€æ´ï¼Œä¸è¦è¾“å‡ºJSONã€‚"""
        
        return AssistantAgent(
            "PathPlanner",
            model_client=self._get_llm_client(),
            system_message=system_message
        )
    
    def create_arbitrator(self) -> AssistantAgent:
        """åˆ›å»ºä»²è£æ™ºèƒ½ä½“"""
        system_message = """ä½ æ˜¯æœ€ç»ˆä»²è£è€…ã€‚
èŒè´£ï¼šç»¼åˆæ‰€æœ‰æ„è§ï¼Œè¾“å‡ºæœ€ç»ˆæ–¹æ¡ˆã€‚
è¦æ±‚ï¼šå¿…é¡»è¾“å‡ºJSONæ ¼å¼çš„æœ€ç»ˆæ–¹æ¡ˆã€‚
è¾“å‡ºæ ¼å¼ï¼š
{
  "final_allocation": {
    "decision_time": "æ—¶é—´æˆ³",
    "total_tasks": 5,
    "total_uavs": 4,
    "assignments": [æ–¹æ¡ˆåˆ—è¡¨],
    "unassigned_tasks": [],
    "total_completion_time": "10:00",
    "risk_assessment": "é£é™©è¯„ä¼°",
    "notes": "å¤‡æ³¨"
  }
}
é‡è¦ï¼šæœ€åå¿…é¡»è¾“å‡ºå®Œæ•´JSONï¼Œè¯´"TERMINATE"ç»“æŸã€‚"""
        
        return AssistantAgent(
            "Arbitrator",
            model_client=self._get_llm_client(),
            system_message=system_message
        )
    
    async def run_configuration(self, config_name: str) -> Dict[str, Any]:
        """è¿è¡ŒæŒ‡å®šé…ç½®çš„å®éªŒ"""
        config = self.configurations[config_name]
        
        print(f"\n{'='*70}")
        print(f"è¿è¡Œé…ç½®: {config['name']}")
        print(f"æ™ºèƒ½ä½“: {', '.join(config['agents'])}")
        print(f"è¯´æ˜: {config['description']}")
        print('='*70)
        
        start_time = time.time()
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agents = []
        agent_creators = {
            'TaskAnalyzer': self.create_task_analyzer,
            'ResourceEvaluator': self.create_resource_evaluator,
            'SolutionGenerator': self.create_solution_generator,
            'ConflictDetector': self.create_conflict_detector,
            'PathPlanner': self.create_path_planner,
            'Arbitrator': self.create_arbitrator
        }
        
        for agent_name in config['agents']:
            agents.append(agent_creators[agent_name]())
        
        # åˆ›å»ºå›¢é˜Ÿ
        termination = MaxMessageTermination(20) | TextMentionTermination("TERMINATE")
        team = RoundRobinGroupChat(agents, termination_condition=termination)
        
        # è¿è¡Œå¯¹è¯
        try:
            result = await team.run(task=self.task_description)
            
            runtime = time.time() - start_time
            
            # æå–ç»“æœ
            allocation_result = self._extract_allocation(result)
            
            if allocation_result:
                # ä¿å­˜ç»“æœ
                output_file = f'{self.output_dir}/allocation_{config_name}.json'
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(allocation_result, f, ensure_ascii=False, indent=2)
                
                print(f"\nâœ… {config['name']} å®Œæˆ")
                print(f"   è¿è¡Œæ—¶é—´: {runtime:.2f}ç§’")
                print(f"   åˆ†é…ä»»åŠ¡: {len(allocation_result.get('final_allocation', {}).get('assignments', []))}/5")
                print(f"   ä¿å­˜åˆ°: {output_file}")
                
                return {
                    'config_name': config_name,
                    'config': config,
                    'result': allocation_result,
                    'runtime': runtime,
                    'success': True
                }
            else:
                print(f"\nâŒ {config['name']} å¤±è´¥ï¼šæœªèƒ½æå–æœ‰æ•ˆåˆ†é…æ–¹æ¡ˆ")
                return {
                    'config_name': config_name,
                    'config': config,
                    'result': None,
                    'runtime': runtime,
                    'success': False
                }
        
        except Exception as e:
            runtime = time.time() - start_time
            print(f"\nâŒ {config['name']} è¿è¡Œå¤±è´¥: {e}")
            return {
                'config_name': config_name,
                'config': config,
                'result': None,
                'runtime': runtime,
                'success': False,
                'error': str(e)
            }
    
    def _extract_allocation(self, result) -> Dict:
        """ä»å¯¹è¯ç»“æœä¸­æå–åˆ†é…æ–¹æ¡ˆ"""
        import re
        
        # è·å–æœ€åå‡ æ¡æ¶ˆæ¯
        messages = []
        for msg in result.messages[-5:]:
            if hasattr(msg, 'content'):
                messages.append(msg.content)
        
        # å°è¯•æå–JSON
        for msg in reversed(messages):
            if isinstance(msg, str):
                # æŸ¥æ‰¾JSONå—
                json_match = re.search(r'\{[\s\S]*"final_allocation"[\s\S]*\}', msg)
                if json_match:
                    try:
                        allocation = json.loads(json_match.group())
                        return allocation
                    except:
                        continue
        
        return None
    
    async def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ"""
        print("â•”" + "â•" * 68 + "â•—")
        print("â•‘" + " " * 20 + "æ™ºèƒ½ä½“æ¶ˆèå®éªŒ" + " " * 28 + "â•‘")
        print("â•š" + "â•" * 68 + "â•")
        print(f"\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nå®éªŒé…ç½®:")
        for i, (config_name, config) in enumerate(self.configurations.items(), 1):
            print(f"  {i}. {config['name']}")
            print(f"     æ™ºèƒ½ä½“æ•°: {len(config['agents'])}")
            print(f"     é¢„æœŸ: {config['expected']}")
        
        # è¿è¡Œæ‰€æœ‰é…ç½®
        for config_name in self.configurations.keys():
            result = await self.run_configuration(config_name)
            self.results[config_name] = result
        
        # è¯„ä¼°æ‰€æœ‰ç»“æœ
        self.evaluate_all_results()
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        self.save_complete_results()
        
        return self.results
    
    def evaluate_all_results(self):
        """è¯„ä¼°æ‰€æœ‰å®éªŒç»“æœ"""
        print(f"\n{'='*70}")
        print("è¯„ä¼°æ‰€æœ‰é…ç½®")
        print('='*70)
        
        for config_name, data in self.results.items():
            if data['success'] and data['result']:
                print(f"\nè¯„ä¼° {data['config']['name']}...")
                
                try:
                    evaluator = AllocationEvaluator(data['result'])
                    metrics = evaluator.evaluate_all()
                    
                    # ä¿å­˜è¯„ä¼°ç»“æœ
                    eval_file = f'{self.output_dir}/evaluation_{config_name}.json'
                    with open(eval_file, 'w', encoding='utf-8') as f:
                        json.dump(metrics, f, ensure_ascii=False, indent=2)
                    
                    data['metrics'] = metrics
                    print(f"   æ€»ä½“è¯„åˆ†: {metrics['overall_score']:.2f}/100")
                
                except Exception as e:
                    print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
                    data['metrics'] = None
            else:
                print(f"\nè·³è¿‡ {data['config']['name']}ï¼ˆè¿è¡Œå¤±è´¥ï¼‰")
                data['metrics'] = None
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print("æ¶ˆèå®éªŒå¯¹æ¯”æŠ¥å‘Š")
        print('='*70)
        
        # è¡¨æ ¼å¤´
        print(f"\n{'é…ç½®':<20} {'æ™ºèƒ½ä½“æ•°':<10} {'æ€»åˆ†':<10} {'ä»»åŠ¡å®Œæˆ':<10} {'æ—¶é—´æ•ˆç‡':<10} {'èµ„æºåˆ©ç”¨':<10}")
        print('-' * 70)
        
        # æ˜¾ç¤ºç»“æœ
        for config_name in self.configurations.keys():
            data = self.results[config_name]
            config = data['config']
            
            if data['success'] and data['metrics']:
                metrics = data['metrics']
                print(f"{config['name']:<20} "
                      f"{len(config['agents']):<10} "
                      f"{metrics['overall_score']:<10.2f} "
                      f"{metrics['task_completion']['completion_rate']:<10.1f} "
                      f"{metrics['time_efficiency']['score']*100:<10.1f} "
                      f"{metrics['resource_utilization']['score']*100:<10.1f}")
            else:
                print(f"{config['name']:<20} "
                      f"{len(config['agents']):<10} "
                      f"{'å¤±è´¥':<10} "
                      f"{'-':<10} "
                      f"{'-':<10} "
                      f"{'-':<10}")
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_config = None
        best_score = -1
        
        for config_name, data in self.results.items():
            if data['success'] and data['metrics']:
                score = data['metrics']['overall_score']
                if score > best_score:
                    best_score = score
                    best_config = (config_name, data)
        
        if best_config:
            print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config[1]['config']['name']}")
            print(f"   æ€»ä½“è¯„åˆ†: {best_score:.2f}/100")
            print(f"   æ™ºèƒ½ä½“æ•°: {len(best_config[1]['config']['agents'])}")
    
    def save_complete_results(self):
        """ä¿å­˜å®Œæ•´å®éªŒç»“æœ"""
        complete_results = {
            'experiment_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'configurations': {},
            'summary': {}
        }
        
        # æ”¶é›†æ‰€æœ‰é…ç½®çš„ç»“æœ
        for config_name, data in self.results.items():
            config_data = {
                'name': data['config']['name'],
                'agents': data['config']['agents'],
                'agent_count': len(data['config']['agents']),
                'description': data['config']['description'],
                'expected': data['config']['expected'],
                'runtime': data['runtime'],
                'success': data['success']
            }
            
            if data['success'] and data['metrics']:
                config_data['metrics'] = {
                    'overall_score': data['metrics']['overall_score'],
                    'task_completion_rate': data['metrics']['task_completion']['completion_rate'],
                    'time_efficiency': data['metrics']['time_efficiency']['score'] * 100,
                    'resource_utilization': data['metrics']['resource_utilization']['score'] * 100,
                    'constraint_satisfaction': data['metrics']['constraint_satisfaction']['score'] * 100
                }
            else:
                config_data['metrics'] = None
            
            complete_results['configurations'][config_name] = config_data
        
        # ç”Ÿæˆæ‘˜è¦
        successful_configs = [(name, data) for name, data in self.results.items() 
                             if data['success'] and data['metrics']]
        
        if successful_configs:
            scores = [data['metrics']['overall_score'] for _, data in successful_configs]
            complete_results['summary'] = {
                'total_experiments': len(self.configurations),
                'successful_experiments': len(successful_configs),
                'max_score': max(scores),
                'min_score': min(scores),
                'avg_score': sum(scores) / len(scores),
                'score_range': max(scores) - min(scores)
            }
            
            # æ‰¾å‡ºæœ€ä½³é…ç½®
            best = max(successful_configs, key=lambda x: x[1]['metrics']['overall_score'])
            complete_results['summary']['best_configuration'] = {
                'name': best[1]['config']['name'],
                'config_key': best[0],
                'agent_count': len(best[1]['config']['agents']),
                'score': best[1]['metrics']['overall_score']
            }
        
        # ä¿å­˜JSON
        output_file = f'{self.output_dir}/ablation_complete_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(complete_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å®Œæ•´å®éªŒç»“æœå·²ä¿å­˜åˆ°: {output_file}")


async def run_ablation_study():
    """è¿è¡Œæ¶ˆèå®éªŒ"""
    experiment = AblationExperiment()
    results = await experiment.run_all_experiments()
    
    print("\n" + "="*70)
    print("âœ¨ æ¶ˆèå®éªŒå®Œæˆï¼")
    print("="*70)
    print("\næŸ¥çœ‹ç»“æœ:")
    print("  â€¢ ablation_results/ablation_complete_results.json (å®Œæ•´æ•°æ®)")
    print("  â€¢ ablation_results/allocation_*.json (å„é…ç½®åˆ†é…æ–¹æ¡ˆ)")
    print("  â€¢ ablation_results/evaluation_*.json (å„é…ç½®è¯„ä¼°ç»“æœ)")
    
    return results


if __name__ == "__main__":
    asyncio.run(run_ablation_study())
