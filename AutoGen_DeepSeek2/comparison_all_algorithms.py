"""
å®Œæ•´å¯¹æ¯”å®éªŒï¼šAutoGen vs 4ç§åŸºçº¿ç®—æ³•
åŒ…æ‹¬ï¼šè´ªå¿ƒç®—æ³•ã€éšæœºåˆ†é…ã€é—ä¼ ç®—æ³•ã€æ•´æ•°è§„åˆ’
"""

import json
import time
from datetime import datetime
from baseline_algorithms import run_baseline_algorithm, TaskAllocationProblem
from evaluation_metrics import AllocationEvaluator
import os
import numpy as np


class AllAlgorithmsComparison:
    """æ‰€æœ‰ç®—æ³•å¯¹æ¯”å®éªŒç®¡ç†ç±»"""
    
    def __init__(self):
        self.results = {}
        self.output_dir = "comparison_results"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        
        # ç®—æ³•é…ç½®
        self.algorithms = {
            'autogen': {'name': 'AutoGen', 'color': '#2E86AB'},
            'greedy': {'name': 'è´ªå¿ƒç®—æ³•', 'color': '#A23B72'},
            'random': {'name': 'éšæœºåˆ†é…', 'color': '#F18F01'},
            'genetic': {'name': 'é—ä¼ ç®—æ³•', 'color': '#C73E1D'},
            'ip': {'name': 'æ•´æ•°è§„åˆ’', 'color': '#6A994E'}
        }
    
    def load_autogen_result(self, filename='output_allocation.json'):
        """åŠ è½½AutoGenç®—æ³•çš„ç»“æœ"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ æœªæ‰¾åˆ°AutoGenç»“æœæ–‡ä»¶: {filename}")
            print("   è¯·å…ˆè¿è¡Œ python autogen_uav_allocation.py ç”Ÿæˆç»“æœ")
            return None
    
    def run_baseline(self, algorithm_name: str) -> dict:
        """è¿è¡ŒåŸºçº¿ç®—æ³•"""
        print(f"\n{'='*70}")
        print(f"è¿è¡Œ {self.algorithms[algorithm_name]['name']} ({algorithm_name.upper()})")
        print('='*70)
        
        start_time = time.time()
        
        problem = TaskAllocationProblem.from_default_scenario()
        result = run_baseline_algorithm(algorithm_name, problem)
        
        runtime = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        output_file = f'{self.output_dir}/allocation_{algorithm_name}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {self.algorithms[algorithm_name]['name']} å®Œæˆ")
        print(f"   è¿è¡Œæ—¶é—´: {runtime:.3f} ç§’")
        print(f"   å®Œæˆä»»åŠ¡: {len(result['final_allocation']['assignments'])}/{result['final_allocation']['total_tasks']}")
        print(f"   ä¿å­˜åˆ°: {output_file}")
        
        return {
            'result': result,
            'runtime': runtime,
            'algorithm': algorithm_name
        }
    
    def evaluate_result(self, result: dict, algorithm_name: str) -> dict:
        """è¯„ä¼°å•ä¸ªç®—æ³•çš„ç»“æœ"""
        print(f"\nè¯„ä¼° {self.algorithms[algorithm_name]['name']}...")
        
        evaluator = AllocationEvaluator(result)
        metrics = evaluator.evaluate_all()
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_file = f'{self.output_dir}/evaluation_{algorithm_name}.json'
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        
        print(f"   æ€»ä½“è¯„åˆ†: {metrics['overall_score']:.2f}/100")
        
        return metrics
    
    def run_all_algorithms(self):
        """è¿è¡Œæ‰€æœ‰ç®—æ³•å¹¶è¯„ä¼°"""
        print("â•”" + "â•" * 68 + "â•—")
        print("â•‘" + " " * 15 + "AutoGen vs æ‰€æœ‰åŸºçº¿ç®—æ³•å¯¹æ¯”å®éªŒ" + " " * 21 + "â•‘")
        print("â•š" + "â•" * 68 + "â•")
        print(f"\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nå¯¹æ¯”ç®—æ³•ï¼š")
        print("  1. AutoGen - å¤šæ™ºèƒ½ä½“åä½œ")
        print("  2. è´ªå¿ƒç®—æ³• - æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡åˆ†é…")
        print("  3. éšæœºåˆ†é… - éšæœºé€‰æ‹©åˆ†é…")
        print("  4. é—ä¼ ç®—æ³• - è¿›åŒ–æœç´¢ä¼˜åŒ–")
        print("  5. æ•´æ•°è§„åˆ’ - æ•°å­¦ä¼˜åŒ–æ±‚è§£")
        
        # 1. åŠ è½½AutoGenç»“æœ
        print(f"\n{'='*70}")
        print("1ï¸âƒ£ åŠ è½½ AutoGen ç®—æ³•ç»“æœ")
        print('='*70)
        
        autogen_result = self.load_autogen_result()
        if autogen_result is None:
            print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°AutoGenç»“æœ")
            print("   è¯·å…ˆè¿è¡Œ: python autogen_uav_allocation.py")
            return None
        
        self.results['autogen'] = {
            'result': autogen_result,
            'runtime': 0,  # AutoGençš„è¿è¡Œæ—¶é—´éœ€è¦å•ç‹¬è®°å½•
            'algorithm': 'autogen'
        }
        print("âœ… AutoGen ç»“æœåŠ è½½æˆåŠŸ")
        
        # 2. è¿è¡Œæ‰€æœ‰åŸºçº¿ç®—æ³•
        baseline_algos = ['greedy', 'random', 'genetic', 'ip']
        
        for i, algo_name in enumerate(baseline_algos, 2):
            print(f"\n{'='*70}")
            print(f"{i}ï¸âƒ£ è¿è¡Œ {self.algorithms[algo_name]['name']}")
            print('='*70)
            self.results[algo_name] = self.run_baseline(algo_name)
        
        # 3. è¯„ä¼°æ‰€æœ‰ç®—æ³•
        print(f"\n{'='*70}")
        print("ğŸ“Š è¯„ä¼°æ‰€æœ‰ç®—æ³•")
        print('='*70)
        
        for algo_name in self.algorithms.keys():
            if algo_name in self.results:
                metrics = self.evaluate_result(self.results[algo_name]['result'], algo_name)
                self.results[algo_name]['metrics'] = metrics
        
        # 4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()
        
        # 5. ä¿å­˜å®Œæ•´å¯¹æ¯”ç»“æœ
        self.save_all_results()
        
        return self.results
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print("ğŸ“ˆ å¯¹æ¯”å®éªŒæŠ¥å‘Š")
        print('='*70)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        print(f"\n{'ç®—æ³•':<12} {'æ€»åˆ†':<10} {'ä»»åŠ¡å®Œæˆ':<10} {'æ—¶é—´æ•ˆç‡':<10} {'èµ„æºåˆ©ç”¨':<10} {'çº¦æŸæ»¡è¶³':<10}")
        print('-' * 70)
        
        for algo_name in self.algorithms.keys():
            if algo_name in self.results:
                algo_display = self.algorithms[algo_name]['name']
                metrics = self.results[algo_name]['metrics']
                
                print(f"{algo_display:<12} "
                      f"{metrics['overall_score']:<10.2f} "
                      f"{metrics['task_completion']['completion_rate']:<10.1f} "
                      f"{metrics['time_efficiency']['score']*100:<10.1f} "
                      f"{metrics['resource_utilization']['score']*100:<10.1f} "
                      f"{metrics['constraint_satisfaction']['score']*100:<10.1f}")
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_algo = max(
            [(name, data['metrics']['overall_score']) 
             for name, data in self.results.items()],
            key=lambda x: x[1]
        )
        
        print(f"\nğŸ† æœ€ä½³ç®—æ³•: {self.algorithms[best_algo[0]]['name']}")
        print(f"   æ€»ä½“è¯„åˆ†: {best_algo[1]:.2f}/100")
        
        # æ’å
        print("\nğŸ“Š ç®—æ³•æ’åï¼š")
        sorted_algos = sorted(
            [(name, data['metrics']['overall_score']) 
             for name, data in self.results.items()],
            key=lambda x: x[1],
            reverse=True
        )
        
        for rank, (algo_name, score) in enumerate(sorted_algos, 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank}."
            print(f"   {medal} {self.algorithms[algo_name]['name']:<12} - {score:.2f}åˆ†")
    
    def save_all_results(self):
        """ä¿å­˜æ‰€æœ‰å¯¹æ¯”ç»“æœ"""
        # åˆ›å»ºå®Œæ•´å¯¹æ¯”æ•°æ®
        comparison_data = {
            'experiment_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'algorithms': list(self.results.keys()),
            'algorithm_names': {k: v['name'] for k, v in self.algorithms.items()},
            'results': {},
            'summary': {}
        }
        
        # æ”¶é›†æ‰€æœ‰ç®—æ³•çš„ç»“æœ
        for algo_name, data in self.results.items():
            metrics = data['metrics']
            comparison_data['results'][algo_name] = {
                'algorithm_name': self.algorithms[algo_name]['name'],
                'overall_score': metrics['overall_score'],
                'task_completion_rate': metrics['task_completion']['completion_rate'],
                'time_efficiency': metrics['time_efficiency']['score'] * 100,
                'resource_utilization': metrics['resource_utilization']['score'] * 100,
                'constraint_satisfaction': metrics['constraint_satisfaction']['score'] * 100,
                'runtime': data['runtime'],
                'completed_tasks': metrics['task_completion']['completed_tasks'],
                'total_tasks': metrics['task_completion']['total_tasks'],
                'detailed_metrics': metrics
            }
        
        # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        scores = [data['metrics']['overall_score'] for data in self.results.values()]
        comparison_data['summary'] = {
            'max_score': max(scores),
            'min_score': min(scores),
            'avg_score': np.mean(scores),
            'std_score': np.std(scores),
            'score_range': max(scores) - min(scores)
        }
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_algo = max(self.results.items(), 
                       key=lambda x: x[1]['metrics']['overall_score'])
        comparison_data['summary']['best_algorithm'] = {
            'name': self.algorithms[best_algo[0]]['name'],
            'code': best_algo[0],
            'score': best_algo[1]['metrics']['overall_score']
        }
        
        # ä¿å­˜JSONæ ¼å¼
        output_file = f'{self.output_dir}/all_algorithms_comparison.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å®Œæ•´å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def generate_latex_table(self):
        """ç”ŸæˆLaTeXæ ¼å¼çš„å¯¹æ¯”è¡¨æ ¼ï¼ˆç”¨äºè®ºæ–‡ï¼‰"""
        latex_code = "\\begin{table}[htbp]\n"
        latex_code += "\\centering\n"
        latex_code += "\\caption{ç®—æ³•æ€§èƒ½å¯¹æ¯”}\n"
        latex_code += "\\begin{tabular}{lcccccc}\n"
        latex_code += "\\hline\n"
        latex_code += "ç®—æ³• & æ€»åˆ† & ä»»åŠ¡å®Œæˆç‡ & æ—¶é—´æ•ˆç‡ & èµ„æºåˆ©ç”¨ & çº¦æŸæ»¡è¶³ & è¿è¡Œæ—¶é—´(ç§’) \\\\\n"
        latex_code += "\\hline\n"
        
        for algo_name in self.algorithms.keys():
            if algo_name in self.results:
                algo_display = self.algorithms[algo_name]['name']
                metrics = self.results[algo_name]['metrics']
                runtime = self.results[algo_name]['runtime']
                
                latex_code += f"{algo_display} & "
                latex_code += f"{metrics['overall_score']:.2f} & "
                latex_code += f"{metrics['task_completion']['completion_rate']:.1f}\\% & "
                latex_code += f"{metrics['time_efficiency']['score']*100:.1f} & "
                latex_code += f"{metrics['resource_utilization']['score']*100:.1f} & "
                latex_code += f"{metrics['constraint_satisfaction']['score']*100:.1f} & "
                latex_code += f"{runtime:.3f} \\\\\n"
        
        latex_code += "\\hline\n"
        latex_code += "\\end{tabular}\n"
        latex_code += "\\end{table}\n"
        
        # ä¿å­˜LaTeXä»£ç 
        latex_file = f'{self.output_dir}/comparison_table.tex'
        with open(latex_file, 'w', encoding='utf-8') as f:
            f.write(latex_code)
        
        print(f"âœ… LaTeXè¡¨æ ¼å·²ä¿å­˜åˆ°: {latex_file}")
        
        return latex_code


def run_full_comparison():
    """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ"""
    exp = AllAlgorithmsComparison()
    results = exp.run_all_algorithms()
    
    if results:
        # ç”ŸæˆLaTeXè¡¨æ ¼
        print(f"\n{'='*70}")
        print("ğŸ“ ç”Ÿæˆè®ºæ–‡è¡¨æ ¼")
        print('='*70)
        exp.generate_latex_table()
        
        print("\n" + "="*70)
        print("âœ¨ å¯¹æ¯”å®éªŒå®Œæˆï¼")
        print("="*70)
        print("\næŸ¥çœ‹ç»“æœ:")
        print("  â€¢ comparison_results/all_algorithms_comparison.json (å®Œæ•´æ•°æ®)")
        print("  â€¢ comparison_results/comparison_table.tex (LaTeXè¡¨æ ¼)")
        print("  â€¢ comparison_results/allocation_*.json (å„ç®—æ³•åˆ†é…æ–¹æ¡ˆ)")
        print("  â€¢ comparison_results/evaluation_*.json (å„ç®—æ³•è¯„ä¼°ç»“æœ)")
        
        return results
    else:
        print("\nâŒ å¯¹æ¯”å®éªŒå¤±è´¥")
        return None


if __name__ == "__main__":
    run_full_comparison()